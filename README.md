<div align="center">
  <h1>üë®‚Äçüíª Awesome Code Benchmark</h1>
  <a href="https://awesome.re">
    <img src="https://awesome.re/badge.svg" alt="Awesome">
  </a>
  <a href="https://img.shields.io/badge/PRs-Welcome-red">
    <img src="https://img.shields.io/badge/PRs-Welcome-red" alt="PRs Welcome">
  </a>
</div>

A comprehensive code domain benchmark review of LLM researches.

<p align="center">
    <img src="https://i.imgur.com/waxVImv.png" alt="Oryx Video-ChatGPT">
</p>

## News 
- üî•üî• **[2025-03-17]** We add **Code Version** (Version-specific code generation) benchmarks.
- üî•üî• **[2025-03-16]** A thorough review of code domain benchmarks for LLM research has been released.

<!-- ## Table of Contents
1. [Code C](#1_Code Completion & Code Generation) -->

## üöÄ Top Code Benchmark

### Code Completion & Code Generation
* **HumanEval**: code completion 
* **MBPP**: text -> code; code generation 
* **APPS**: a benchmark for code generation from natural language specifications
* **EvalPlus**: Extends the HumanEval and MBPP benchmarks
* **MultiPL-E**: Extends the HumanEval and MBPP benchmarks to 18 languages
* **CodeClarQA**: containing pairs of natural language descriptions and code with created synthetic clarification questions and answers.
* **DevEval**: Repo-level code generation
* **BigCodeBench**: Complete Split & Instruct Split
* **DynaCode**: a dynamic complexity-aware code benchmark

| Benchmark | Paper | Date | Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| HumanEval     | [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)                                                                       | Arxiv 2021/07       | [Github](https://github.com/openai/human-eval)                                | [ü§óDataset](https://huggingface.co/datasets/openai/openai_humaneval) | 
| MBPP          | [Program Synthesis with Large Language Models](https://arxiv.org/abs/2108.07732)                                                                           | Arxiv 2021/08       | | [ü§óDataset](https://huggingface.co/datasets/google-research-datasets/mbpp) | 
| APPS          | [Measuring Coding Challenge Competence With APPS](https://arxiv.org/abs/2105.09938)                                                                        | NeurIPS 2021        | [Github](https://github.com/hendrycks/apps)                                   | [ü§óDataset](https://huggingface.co/datasets/codeparrot/apps) |
| EvalPlus      | [Is Your Code Generated by Chat{GPT} Really Correct? Rigorous Evaluation of Large Language Models for Code Generation](https://arxiv.org/abs/2305.01210)   | NeurIPS 2023        | [Github](https://github.com/evalplus/evalplus)                                | [ü§óDataset](https://huggingface.co/evalplus) | 
| MultiPL-E     | [MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation](https://ieeexplore.ieee.org/abstract/document/10103177)               | TSE 2023            | [Github](https://github.com/nuprl/MultiPL-E)                                  | [ü§óDataset](https://huggingface.co/datasets/nuprl/MultiPL-E) |
| CodeClarQA    | [Python Code Generation by Asking Clarification Questions](https://arxiv.org/abs/2212.09885v2)                                                             | ACL 2023            | [Github](https://github.com/UKPLab/codeclarqa)                                | [Dataset](https://drive.google.com/file/d/1bM-b-L10vNpk7Onyft9BXK8GlMIGl52q/view?usp=sharing) | 
| DevEval       | [DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories](https://arxiv.org/abs/2405.19856)                      | ACL 2024            | [Github](https://github.com/seketeam/DevEval)                                 | [ü§óDataset](https://huggingface.co/datasets/LJ0815/DevEval/blob/main/Source_Code.tar.gz)|
| BigCodeBench  | [BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions](https://arxiv.org/abs/2406.15877)                        | ICLR 2025           | [Github](https://github.com/bigcode-project/bigcodebench)                     | [üìäLeaderBoard](https://huggingface.co/spaces/bigcode/bigcodebench-leaderboard) | ¬† 
| DynaCode      | [DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation](https://arxiv.org/abs/2503.10452)            | Arxiv 2025-03       | | |


* RepoFusion: Training Code Models to Understand Your Repository
* STUDENTEVAL: A Benchmark of Student-Written Prompts for Large Language Models of Code
* MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages
* LongCoder: A Long-Range Pre-trained Language Model for Code Completion
* RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems
* ReCode Robustness Evaluation of Code Generation Models
* LONGBENCH: A BILINGUAL, MULTITASK BENCHMARK FOR LONG CONTEXT UNDERSTANDING
* OctoPack Instruction Tuning Code Large Language Models
* Program Synthesis with Large Language Models
* COCO: Testing Code Generation Systems via Concretized Instructions
* Execution-Based Evaluation for Open-Domain Code Generation
* BIOCODER: A BENCHMARK FOR BIOINFORMATICS CODE GENERATION WITH CONTEXTUAL PRAGMATIC KNOWLEDGE
* CROSSCODEEVAL: A Diverse and Multilingual Benchmark for Cross-File Code Completion
* Large Language Models of Code Fail at Completing Code with Potential Bugs*
* MT-Bench
* ML-BENCH: LARGE LANGUAGE MODELS LEVERAGE OPEN-SOURCE LIBRARIES FOR MACHINE LEARNING TASKS
* PLPilot: Benchmark an Automated Programming Language Design Framework Enabled by Large Language Models
* CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models
* Improving Natural Language Capability of Code Large Language Model
* StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback
* OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models
* A Static Evaluation of Code Completion by Large Language Models
* L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models
* ICE-Score: Instructing Large Language Models to Evaluate Code
* Exploring Language Model‚Äôs Code Generation Ability with Auxiliary Functions
* R2E: TURNING ANY GITHUB REPOSITORY INTO A PROGRAMMING AGENT TEST ENVIRONMENT
* Evaluating Large Language Models with Runtime Behavior of Program Execution
* InfiCoder-Eval: Systematically Evaluating the Question-Answering Capabilities of Code Large Language Models
* Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large Language Model Code Generation
* EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories
* CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks
* Exploring and Evaluating Hallucinations in LLM-Powered Code Generation  (Hallucination benchmark?)
* LeetCodeEval
* Exploring Multi-Lingual Bias of Large Code Models in Code Generation
* CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification (Hallucination benchmark)
* Coeditor Leveraging Contextual Changes for Multi-round Code Auto-editing
* CodeContests
* Top Leaderboard Ranking = Top Coding Proficiency, Always? EVOEVAL: Evolving Coding Benchmarks via LLM
* LLM4Decompile: Decompiling Binary Code with Large Language Models  
* Enhancing Repository-Level Code Generation with Integrated Contextual Information (Rust)
* AICoderEval: Improving AI Domain Code Generation of Large Language Models
* CODEAGENT: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges
* AssertionBench: A Benchmark to Evaluate Large-Language Models for Assertion Generation
* Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
* GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding
* ConCodeEval: Evaluating Large Language Models for Code Constraints in Domain-Specific Languages
* Classeval:A manually-crafted benchmark for evaluating llms on class-level code generation.
* MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation
* Assessing Programming Task Difficulty for Efficient Evaluation of Large Language Models
* CodeMirage: Hallucinations in Code Generated by Large Language Models ÔºàHallucinationÔºâ
* DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation
* On the Impacts of Contexts on Repository-Level Code Generation
* Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion
* LLMs cannot find reasoning errors, but can correct them given the error location
* Reflexion: Language Agents with Verbal Reinforcement Learning
* Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation
* Showing LLM-Generated Code Selectively Based on Confidence of LLMs
* Taco: Topics in algorithmic code generation dataset
* COLLU-BENCH: A BENCHMARK FOR PREDICTING LANGUAGE MODEL HALLUCINATIONS IN CODE (hallucination)
* EVOR: Evolving Retrieval for Code Generation
* PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs
* HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation
* On leakage of code generation evaluation datasets.
* CODEELO: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings
* Codescore: Evaluating code generation by learning code execution
* Deep-Bench: Deep Learning Benchmark Dataset for Code Generation
* Can Language Models Replace Programmers? REPOCOD Says ‚ÄòNot Yet‚Äô
* JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models

### Code Efficiency
| Benchmark | Paper | Date | Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| EvalPerf      | [Evaluating Language Models for Efficient Code Generation](https://arxiv.org/abs/2408.06450)                                          | COLM 2024                   | [Github](https://github.com/evalplus/evalplus)        | [ü§óDataset](https://huggingface.co/datasets/evalplus/evalperf) |
| EffiBench     | [EffiBench: Benchmarking the Efficiency of Automatically Generated Code](https://arxiv.org/abs/2402.02037)                            | NeurIPS 2024                | [Github](https://github.com/huangd1999/EffiBench)     |  |
| Mercury       | [Mercury: A Code Efficiency Benchmark for Code Large Language Models](https://arxiv.org/abs/2402.07844v4)                             | NeurIPS 2024                | [Github](https://github.com/Elfsong/Mercury)          | [ü§óDataset](https://huggingface.co/datasets/Elfsong/Mercury) |
| ECCO          | [ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?](https://arxiv.org/abs/2407.14044)  | EMNLP 2024                  | [Github](https://github.com/CodeEff/ECCO)             | [ü§óDataset](https://huggingface.co/datasets/CodeEff/ECCO)|
| PIE           | [Learning Performance-Improving Code Edits](https://arxiv.org/abs/2302.07867)                                                         | ICLR 2024                   | [Github](https://github.com/LearningOpt/pie)          | [üåêWebsite](https://pie4perf.com)|  
| ENAMEL        | [How Efficient is LLM-Generated Code? A Rigorous & High-Standard Benchmark](https://arxiv.org/abs/2406.06647)                         | ICLR 2025                   | [Github](https://github.com/q-rz/enamel)              | [ü§óDataset](https://huggingface.co/datasets/q-rz/enamel) |

* COFFE: A Code Efficiency Benchmark for Code Generation

### CodeFix & Bug-Fix
* **HumanEvalFix**: code repair capabilitie 
* **SWT-Bench**: Evaluating LLMs on testing generation for real world software issues 
* **SWE-bench**: Evaluating LLMs Resolve Real-World GitHub Issues 
* **SWE-bench Multimodal**: Evaluate LLMs on their ability to fix bugs in visual, user-facing JavaScript software 

| Benchmark | Paper | Date | Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| HumanEvalFix          | [OctoPack: Instruction Tuning Code Large Language Models](https://arxiv.org/abs/2308.07124)                                    | Arxiv 2023/08              | [Github](https://github.com/bigcode-project/octopack)        | [ü§óDataset](https://huggingface.co/datasets/bigcode/humanevalpack) |  
| SWT-Bench             | [SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents](https://arxiv.org/abs/2406.12952)                    | NeurIPS 2024               | [Github](https://github.com/logic-star-ai/SWT-Bench)         | [üåêWebsite](https://swtbench.com) |
| SWE-bench             | [SWE-bench: Can Language Models Resolve Real-World GitHub Issues?](https://arxiv.org/abs/2310.06770)                           | ICLR 2024                  | [Github](https://github.com/swe-bench/SWE-bench)             | [üåêWebsite](https://www.swebench.com) | 
| SWE-bench Multimodal  | [SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?](https://arxiv.org/abs/2410.03859)                 | ICLR 2025                  | [Github](https://github.com/swe-bench/SWE-bench)             | [üåêWebsite](https://www.swebench.com/multimodal) [ü§óDataset](https://www.swebench.com/multimodal) | 

*  Defects4J.
* What‚Äôs Wrong with Your Code Generated by Large Language Models? An Extensive Study

### Code Reasoning & Understanding
* **CRUXEval**: code reasoning, understanding, and execution capabilities
* **CodeMMLU**: code understanding and comprehension

|Benchmark | Paper | Date | Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| CRUXEval      | [CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution](https://arxiv.org/abs/2401.03065)                                      | Arxiv 2024/01         | [Github](https://github.com/facebookresearch/cruxeval)        | [üìäLeaderBoard](https://crux-eval.github.io/leaderboard.html) |  ¬† 
| CodeMMLU      | [CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs](https://arxiv.org/abs/2410.01999)                 | ICLR 2025             | [Github](https://github.com/FSoft-AI4Code/CodeMMLU/)          | [ü§óDataset](https://huggingface.co/datasets/Fsoft-AIC/CodeMMLU) [üìäLeaderBoard](https://fsoft-ai4code.github.io/leaderboards/codemmlu/) [üåê  Website](https://fsoft-ai4code.github.io/codemmlu/) | 

* CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation
* CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?
* CODE-VISION: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities
* SpecEval: Evaluating Code Comprehension in Large Language Models via Program Specifications

### Data science
* **DS-1000**: Data Science Code Generation
* **DA-Code**: Data science tasks 

| Benchmark | Paper | Date | Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| DS-1000      | [DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation](https://arxiv.org/abs/2211.11501)                                      | ICML 2023                 | [Github](https://github.com/xlang-ai/DS-1000)        | [üåêHomePage](https://ds1000-code-gen.github.io) [ü§óDataset](https://huggingface.co/datasets/xlangai/DS-1000)  | 
| DA-Code      | [DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models](https://arxiv.org/abs/2410.07331)                                 | EMNLP 2024                | [Github](https://github.com/yiyihum/da-code)         | [üåêWebsite](https://da-code-bench.github.io) [ü§óDataset](https://huggingface.co/datasets/Jianwen2003/DA-Code) | 

* Natural Language to Code Generation in Interactive Data Science Notebooks

### Text2SQL 
* **Spider**:  text-to-SQL  
* **Spider 2.0**: text-to-SQL

| Benchmark | Paper | Date | Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| Spider      | [Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task](https://arxiv.org/abs/1809.08887)         | EMNLP 2018     | [Github](https://github.com/taoyds/spider)        | [üåêHomepage](https://yale-lily.github.io/spider) |
| Spider 2.0  | [Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows](https://arxiv.org/abs/2411.07763)                                  | ICLR 2025      | [Github](https://github.com/xlang-ai/Spider2)     | [üåêWebsite](https://spider2-sql.github.io) |


### MultiModal Code Generation
* **ChartMimic** : Chart-to-Code Generation

| Benchmark | Paper | Date| Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| ChartMimic      | [ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation](https://arxiv.org/abs/2406.09961)                       | ICLR 2025                | [Github](https://github.com/ChartMimic/ChartMimic)        | [üåêWebsite](https://chartmimic.github.io) [ü§óDataset](https://huggingface.co/datasets/ChartMimic/ChartMimic) |  

* MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems
* Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots
* Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs
* ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code Generation


### Security Code Generation
*  **RedCode**: comprehensive and practical evaluations on the safety of code agents 

| Benchmark | Paper | Date| Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| RedCode        | [RedCode: Risky Code Execution and Generation Benchmark for Code Agents](https://arxiv.org/abs/2411.07781)                                      | NeurIPS 2024                | [Github](https://github.com/AI-secure/RedCode)        | [üåêWebsite](https://redcode-agent.github.io) [üìäLeaderBoard](https://redcode-agent.github.io/#leaderboard) |

* Benchmarking the Security Aspect of Large Language Model-Based Code Generation
* TruthfulQA
* Tim Boland and Paul E. Black. Juliet 1.1 C/C++ and java test suite. Computer, 45(10):88‚Äì90, 2012.
* Purple llama cyberseceval: A secure coding benchmark for language models.
* NExT: Teaching Large Language Models to Reason about Code Execution
* Can LLMs Patch Security Issues?
* SECCODEPLT: A UNIFIED PLATFORM FOR EVALUATING THE SECURITY OF CODE GENAI

### Code Translation
* **TransCoder**: code translation in C++, Java, Python


| Benchmark | Paper | Date| Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| TransCoder        | [Unsupervised Translation of Programming Languages](https://arxiv.org/abs/2006.03511)                                      | NeurIPS 2020                | [Github](https://github.com/facebookresearch/TransCoder)(deprecated) [Github](https://github.com/facebookresearch/CodeGen)(new)        |  |


### Code Version
Version-specific code generation
* **CodeUpdateEval**: code migration with Time-wise dataset                        
* **JavaVersionGenBench**: Code Completion Across Evolving JAVA Versions                
* **VersiCode**: Version-controllable Code Generation                         
* **GitChameleon**: 116 version-aware Python code-completion problems with unit tests 
* **LLM-Deprecated-APl**:  Deprecated APl mapping and functions code completion        
* **CodeUpdateArena**: API Update Knowledge Editing Assessment                      
* **LibEvolutionEval**: Version-Specifc Code Generation                              

| Benchmark | Paper | Date| Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| CodeUpdateEval             | [Automatically Recommend Code Updates: Are We There Yet?](https://arxiv.org/abs/2209.07048v3)                                                |  TOSEM 2024      | [Github](https://github.com/yueyueL/CodeLM-CodeUpdateEval)                       | [ü§óDataset](https://github.com/yueyueL/CodeLM-CodeUpdateEval/tree/main/dataset) | 
| JavaVersionGenBench        | [On the Generalizability of Deep Learning-based Code Completion Across Programming Language Versions](https://arxiv.org/pdf/2403.15149)      |  ICPC 2024       | [Github](https://github.com/java-generalization/java-generalization-replication) | [ü§óDataset](https://zenodo.org/records/10057237)               | 
| VersiCode                  | [VersiCode: Towards Version-controllable Code Generation](https://arxiv.org/abs/2406.07411)                                                  |  Arxiv 2024/10   | [Github](https://github.com/wutong8023/VersiCode)                                | [üåêWebsite](https://wutong8023.site/VersiCode/) [ü§óDataset](https://huggingface.co/datasets/AstoneNg/VersiCode) | 
| GitChameleon               | [GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models](https://arxiv.org/abs/2411.05830)                     |  Arxiv 2024/11   | [Github](https://github.com/NizarIslah/GitChameleon)                             | [ü§óDataset](https://github.com/NizarIslah/GitChameleon/tree/main/dataset) | 
| LLM-Deprecated-APl         | [LLMs Meet Library Evolution: Evaluating Deprecated API Usage in LLM-based Code Completion](https://arxiv.org/abs/2406.09834)                |  ICSE 2025       | [Github](https://github.com/cs-wangchong/LLM-Deprecated-API)                     | [ü§óDataset](https://figshare.com/s/e8de860d8fc2ec0541d2)       |
| CodeUpdateArena            | [CodeUpdateArena: Benchmarking Knowledge Editing on API Updates](https://arxiv.org/abs/2407.06249)                                           |  Arxiv 2025/02   | [Github](https://github.com/leo-liuzy/CodeUpdateArena)                           | [ü§óDataset](https://github.com/leo-liuzy/CodeUpdateArena/tree/main/data) | 
| LibEvolutionEval           | [LibEvolutionEval: A Benchmark and Study for Version-Specific Code Generation](https://arxiv.org/abs/2412.04478)                             |  NAACL 2025      |                                                                                  |                                                              | 


### Industry Code Generation
PLC (Programmable logic controller) & Verilog (Hardware description language) & ... (to be released soon)

* VerilogEval Evaluating Large Language Models for Verilog Code Generation

### Multi-Dimension
* **LiveCodeBench**: self-repair, code execution, test output prediction, code generation
* **RACE**: Readability, Maintainability, Correctness, and Efficiency 

| Benchmark | Paper | Date| Github | Dataset & Website & LeaderBoard |
|:--|:--|:--|:--|:--|
| LiveCodeBench | [LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code](https://arxiv.org/abs/2403.07974)  | Arxiv 2024/03 | [Github](https://github.com/LiveCodeBench/LiveCodeBench) | [ü§óDataset](https://huggingface.co/livecodebench) |  
| RACE          | [Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models](https://arxiv.org/abs/2407.11470) | Arxiv 2024/07 | [Github](https://github.com/jszheng21/RACE)              | [üìäLeaderBoard](https://huggingface.co/spaces/jszheng/RACE_leaderboard) | 

* CODEEDITORBENCH: EVALUATING CODE EDITING CAPABILITY OF LARGE LANGUAGE MODELS
* AnalogCoder: Analog Circuit Design via Training-Free Code Generation
* FairCode: Evaluating Social Bias of LLMs in Code Generation ÔºàbiasÔºâ
* CODERAG-BENCH: Can Retrieval Augment Code Generation?
* LEARNING CODE PREFERENCE VIA SYNTHETIC EVOLUTION
* Judgebench: A benchmark for evaluating llm-based judges.
* CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models
* CodeArena: A Collective Evaluation Platform for LLM Code Generation (evaluation platform)
* TESTEVAL: Benchmarking Large Language Models for Test Case Generation
* InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models
* TestBench: Evaluating Class-Level Test Case Generation Capability of Large Language Models